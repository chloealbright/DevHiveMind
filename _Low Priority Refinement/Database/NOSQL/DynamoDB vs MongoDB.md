---
tags: 
author:
  - jacgit18
Status: 
Started: 
EditDate: 
Relates:
---
Selecting transcript lines in this section will navigate to timestamp in the video 

How does DynamoDB compare with MongoDB? Well let's talk about Mongo first. Mongo was founded in 2007 by engineers and executives from Google and other tech companies. And it's managed by a company called MongoDB, Inc. It is a great document database, and your project could be better suited for Dynamo or Mongo, so it's good to know the difference between the two so you can make the right choice. Number one, DynamoDB works best as a key/value store. It stores data in rows of dynamic columns. Mongo stores data in documents inside of collections. A document can be up to 16 megabytes in size, where a row in DynamoDB caps out at about 400k. So that's important to know. DynamoDB has limited scalar types. Numbers, strings, Booleans. Anything more complicated needs to be saved as a string and then converted in your code. Mongo supports date, timestamp and more advanced fields. You may or may not need this, and this may or may not be important to you, but that's a decision you'll have to make. Where DynamoDB excels is pricing. DynamoDB is priced on throughput, meaning read and write units. And we'll talk in a future video about how that's calculated. But DynamoDB offers 25 gigabytes of storage up front, and it's a fully managed service. Now, MongoDB has MongoDB Atlas, which is their version of a fully managed service, and you're going to pay a monthly service plan, except if you go with the free version which gives you 500 megabytes, but there's no uptime SLA, so it's really not for production. You'll need to go to the MongoDB Atlas website to learn more about this service, because it's beyond the scope of this course. When I think DynamoDB Fits is when you're planning on using this database as a key/value store, you only want to pay for the throughput that you're using, you want to have quick auto scaling at the click of a button, and you want to have maximum integration with AWS. Now, if you're familiar with MongoDB and you want to learn the terminology differences, where in MongoDB stores documents in a collection, DynamoDB stores items in a table. And those items are represented as columns. Those column values are called attributes, where in a MongoDB document they would be called fields.





Data modeling in DynamoDB 

you're coming from a Relational SQL database background, modeling your data to get the most out of DynamoDB requires a little shift in thinking. In this video, I'm going to tell you how to spread your data across partitions, and I'm going to explain what we mean by a cold partition or a hot partition, and why you'll get so much benefit out of understanding this. Let's look at a traditional relational database schema for capturing AtBats in a baseball game. Remember, the baseball game has teams, players, games, and every time a batter comes to the plate, that's considered an AtBat. As you can see, we have all the elements represented here to store a log of AtBats for a professional baseball game. Now you might be tempted to create four tables in DynamoDB to represent the same data, but that's not how DynamoDB works. Ideally, you should have one table for your entire application, and explain why that's going to help you, you need to understand how DynamoDB gets its performance benefit. Every item in DynamoDB requires at least one attribute, the partition key. When you provide your item and partition key, DynamoDB hashes your key and uses that hash value as the memory address for your data. Imagine if the blocks below are different memory address spaces. I'm using Team ID as my partition. Watch what happens when I put my data into the table. The key is hashed and a memory address is assigned to that value. When I fetch the data, the same thing happens. Remember, DynamoDB is basically a key store that's highly efficient at putting and retrieving data by a given key. So selecting our partition key is the first task in data modeling. To get that single digit millisecond performance we're after, we have to try and avoid one partition becoming too hot, and to demonstrate what I mean by a hot partition, let's look at a bad example. A typical bad partition key for a table would be the date, and here's why. This has an impact on performance because there are so many resources that can access that same address space at once. This is called a hot key and it's bad and you want to avoid it. Some other examples of bad partition keys would be Order IDs and shipping numbers. Dates, we've already explained, because once they're in the past, those partitions won't get used very much. If you're using a counter for order numbers, those original keys are going to get cold real fast and never accessed again and all the activity is going to happen on a small group of partition keys. What we're after is that your partition key should be based on something regularly accessed and spread across all nodes. We want requests spread evenly, so let's go back to our baseball example, and see how our partition keys' going to help do this. There are 30 teams in major league baseball, each participating in a game every week. If I'm recording stats about all of them, wouldn't it make sense to use a partition key based on the Team ID and not the date? Let's look at our model again. Now if we store our data based on Team ID, I should get an even distribution across all partitions, which is what we want.
